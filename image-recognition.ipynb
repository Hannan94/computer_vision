{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # A Computer Vision Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. \n",
    "\n",
    "**Labels**\n",
    "\n",
    "Each training and test example is assigned to one of the following labels:    \n",
    "\n",
    "0 T-shirt/top  \n",
    "1 Trouser  \n",
    "2 Pullover  \n",
    "3 Dress  \n",
    "4 Coat  \n",
    "5 Sandal  \n",
    "6 Shirt  \n",
    "7 Sneaker  \n",
    "8 Bag  \n",
    "9 Ankle boot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Fashion-mnist dataset \n",
    "mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainin & Testing the data\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[  0   0   0   1   0   0   0   0   0  34 148 202 107  84 162 198  90  26   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  21 107 166 168 152 204 238 248 237 190 142 178 158  98  17   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0  55 188 140 131 121 119 116 146 157 132 115 136 125 140 143 181  51   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 167 134 113 130 134 140 132 125 125 131 135 130 133 131 115 135 163   6   0   0   0   0   0]\n",
      " [  0   0   0   0  58 148 133 124 134 128 135 128 133 130 133 137 138 130 124 130 133 147  82   0   1   0   0   0]\n",
      " [  0   0   0   0  86 148 158 139 124 120 131 134 132 134 124 134 132 137 126 137 136 146 114   0   0   0   0   0]\n",
      " [  0   0   0   0 112 141 174 143 117 134 130 128 131 136 134 128 128 131 106 187 149 147 143   0   0   0   0   0]\n",
      " [  0   0   0   0 134 151 167 154 114 134 138 139 135 135 130 130 124 129 119 229 141 145 164   0   0   0   0   0]\n",
      " [  0   0   0   0 136 154 169 157 109 147 140 137 137 136 142 141 139 129 123 201 146 138 172  21   0   0   0   0]\n",
      " [  0   0   0   0 145 158 171 175 111 136 145 136 135 138 137 143 146 140 122 184 167 136 170  58   0   0   0   0]\n",
      " [  0   0   0   0 151 155 189 183 120 135 132 136 135 133 128 130 134 120 153 230 156 135 167  83   0   0   0   0]\n",
      " [  0   0   0   1 166 151 208 175 116 142 144 138 149 133 131 137 140 119 126 206 198 141 166 101   0   0   0   0]\n",
      " [  0   0   0   6 169 146 237 187 104 131 136 149 147 137 137 137 132 137 113 166 206 151 162 126   0   0   0   0]\n",
      " [  0   0   0  20 180 136 231 214 105 134 140 139 136 137 136 148 134 149 111 139 255 151 158 147   0   0   0   0]\n",
      " [  0   0   0  33 184 109 217 182 105 124 143 131 135 137 139 138 131 142 132  99 249 162 157 172   0   0   0   0]\n",
      " [  0   0   0  35 180 112 233 143 109 133 145 146 142 142 139 141 135 141 152 101 205 163 154 170   0   0   0   0]\n",
      " [  0   0   0  42 183 123 217 117 126 137 140 134 137 135 143 141 133 133 149 107 166 176 153 169  14   0   0   0]\n",
      " [  0   0   0  48 175 147 210 101 119 136 142 137 142 139 147 142 146 126 138 102 131 197 161 183  32   0   0   0]\n",
      " [  0   0   0  56 165 163 213  74 126 138 149 133 133 132 141 134 142 132 132 126 117 193 162 172  39   0   0   0]\n",
      " [  0   0   0  62 157 165 227  39 135 138 136 142 146 143 136 138 134 141 134 116  78 197 171 156  44   0   0   0]\n",
      " [  0   0   0  65 162 169 245  39 140 133 133 128 132 134 143 145 134 133 134 128  36 208 172 151  55   0   0   0]\n",
      " [  0   0   0  72 147 168 244  25 142 137 138 133 128 132 147 145 135 136 133 131  26 207 173 149  67   0   0   0]\n",
      " [  0   0   0  68 153 179 250  11 135 134 139 133 136 140 140 143 145 140 131 126  32 207 175 136  73   0   0   0]\n",
      " [  0   0   0  61 155 180 252  23 151 132 131 133 133 129 135 140 132 139 132 139  38 221 174 135  77   0   0   0]\n",
      " [  0   0   0  18 147 187 185   3 173 153 151 150 153 150 150 153 148 151 146 161   4 184 175 136  37   0   0   0]\n",
      " [  0   0   0   0 150 204  76   0 213 165 167 170 172 171 169 170 172 167 160 202   0 107 196 152   0   0   0   0]\n",
      " [  0   0   0   0 123 162  41   2 153 124 133 136 141 135 132 134 139 144 141 132   0  67 153 142   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot a data sample (i.e an image)\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=200)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(training_images[45])\n",
    "print(training_labels[45])\n",
    "print(training_images[45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the number are between 0 and 255. If we are training a neural network, for various reasons it's easier if we treat all values as between 0 and 1, a process called 'normalizing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images  = training_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design the model\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_23 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Neural Network designing\n",
    "model = keras.Sequential([tf.keras.layers.Flatten(input_shape = (28, 28)), #image shape is 28 X 28. We are specifying here to expect the input by 28 x 28.\n",
    "                          tf.keras.layers.Dense(128, activation = tf.nn.relu),\n",
    "                          tf.keras.layers.Dense(10, activation = tf.nn.softmax) #10 is the number of classes in dataset\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential**: That defines a SEQUENCE of layers in the neural network\n",
    "\n",
    "**Flatten**: Remember earlier where our images were a square, when you printed them out? Flatten just takes that square and turns it into a 1 dimensional set.\n",
    "\n",
    "**Dense**: Adds a layer of neurons\n",
    "\n",
    "Each layer of neurons need an **activation function** to tell them what to do. There's lots of options, but just use these for now. \n",
    "\n",
    "**Relu** effectively means \"If X>0 return X, else return 0\" -- so what it does it it only passes values 0 or greater to the next layer in the network.\n",
    "\n",
    "**Softmax** takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0] -- The goal is to save a lot of coding!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.5014 - accuracy: 0.8238\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3774 - accuracy: 0.8634\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.3376 - accuracy: 0.8763\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.3110 - accuracy: 0.8858\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.2950 - accuracy: 0.8920\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.2826 - accuracy: 0.8953\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.2674 - accuracy: 0.9005\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.2584 - accuracy: 0.9042\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.2479 - accuracy: 0.9070\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.2417 - accuracy: 0.9090\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.2317 - accuracy: 0.9129\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.2250 - accuracy: 0.9152\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.2182 - accuracy: 0.9178\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2130 - accuracy: 0.9186\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2073 - accuracy: 0.9221s - loss: 0.2075 - accuracy: 0.92\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2024 - accuracy: 0.9244\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1970 - accuracy: 0.9262\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1909 - accuracy: 0.9281\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1864 - accuracy: 0.9285\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1805 - accuracy: 0.9314\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.1768 - accuracy: 0.9326\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1733 - accuracy: 0.9342\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1700 - accuracy: 0.9355\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1639 - accuracy: 0.9379\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1601 - accuracy: 0.9392\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1570 - accuracy: 0.9405\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1536 - accuracy: 0.9426\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.1492 - accuracy: 0.9434\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1472 - accuracy: 0.9440\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1463 - accuracy: 0.9448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x64e680fd0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.3613 - accuracy: 0.8860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36134741147756577, 0.886]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model evaluation\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.7420787e-11 1.1006636e-13 6.2703786e-13 1.3650139e-18 1.7614985e-12 1.6893561e-04 4.4193991e-14 6.1535265e-04 2.3089564e-14 9.9921572e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#gives the the probability that this item is each of the 10 classes\n",
    "classifications = model.predict(test_images)\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Callback to stop Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.4956 - accuracy: 0.8246\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.3730 - accuracy: 0.8650\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.3348 - accuracy: 0.8780\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.3126 - accuracy: 0.8852\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2929 - accuracy: 0.8918\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2801 - accuracy: 0.8967\n",
      "Epoch 7/15\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 0.2653 - accuracy: 0.9012\n",
      "Reached 90% accuracy so cancelling training!\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2658 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 21us/sample - loss: 0.3511 - accuracy: 0.8772\n",
      "[2.0990917e-06 2.2640103e-08 1.3938508e-08 4.2507131e-09 1.6742990e-09 1.4683582e-02 4.9993400e-07 3.2747701e-02 3.7028985e-07 9.5256573e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.90):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images/255.0\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=15, callbacks=[callbacks])\n",
    "model.evaluate(test_images, test_labels)\n",
    "classifications = model.predict(test_images)\n",
    "print(classifications[0])\n",
    "print(test_labels[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Accuracy is 90% on training and 87% on validation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Computer Vision Accuracy using Convolutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 31s 513us/sample - loss: 0.4427 - accuracy: 0.8386\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 32s 533us/sample - loss: 0.2917 - accuracy: 0.8927\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 32s 534us/sample - loss: 0.2437 - accuracy: 0.9097\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 32s 532us/sample - loss: 0.2119 - accuracy: 0.9213\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 30s 502us/sample - loss: 0.1862 - accuracy: 0.9304\n",
      "10000/10000 [==============================] - 1s 101us/sample - loss: 0.2728 - accuracy: 0.9048\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)), #convolution layer\n",
    "  tf.keras.layers.MaxPooling2D(2, 2), #pooling layer\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),#convolution layer\n",
    "  tf.keras.layers.MaxPooling2D(2,2),#pooling layer\n",
    "  tf.keras.layers.Flatten(), #input layer\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')#output layer\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's likely gone up to about 93% on the training data and 91% on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the training and testing data\n",
    "First convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.1604 - accuracy: 0.9528\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 11s 188us/sample - loss: 0.0529 - accuracy: 0.9839\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0352 - accuracy: 0.9890\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 12s 200us/sample - loss: 0.0221 - accuracy: 0.9929\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 12s 199us/sample - loss: 0.0162 - accuracy: 0.9946\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 12s 200us/sample - loss: 0.0104 - accuracy: 0.9966\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 12s 200us/sample - loss: 0.0086 - accuracy: 0.9972\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.0061 - accuracy: 0.9980\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 191us/sample - loss: 0.0061 - accuracy: 0.9980\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 190us/sample - loss: 0.0043 - accuracy: 0.9986\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0491 - accuracy: 0.9877\n",
      "0.9877\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**32 Convolutions** accuracy is 99.8% and validation is 98.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 17s 286us/sample - loss: 0.1331 - accuracy: 0.9600\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 17s 281us/sample - loss: 0.0468 - accuracy: 0.9856\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 17s 287us/sample - loss: 0.0294 - accuracy: 0.9912\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 17s 285us/sample - loss: 0.0200 - accuracy: 0.9931\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 17s 286us/sample - loss: 0.0122 - accuracy: 0.9959\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 17s 286us/sample - loss: 0.0101 - accuracy: 0.9967\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 18s 293us/sample - loss: 0.0068 - accuracy: 0.9976\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 18s 294us/sample - loss: 0.0064 - accuracy: 0.9979\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 18s 294us/sample - loss: 0.0048 - accuracy: 0.9985\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 17s 283us/sample - loss: 0.0045 - accuracy: 0.9984\n",
      "10000/10000 [==============================] - 1s 74us/sample - loss: 0.0902 - accuracy: 0.9805\n",
      "0.9805\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **64 Convolutions** accuracy is 99.8% and validation is 98.05%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 8s 136us/sample - loss: 0.1735 - accuracy: 0.9489\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.0589 - accuracy: 0.9816\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 8s 129us/sample - loss: 0.0378 - accuracy: 0.9882\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 0.0260 - accuracy: 0.9919\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.0178 - accuracy: 0.9940\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0126 - accuracy: 0.9959\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.0103 - accuracy: 0.9964\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.0070 - accuracy: 0.9978\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.0057 - accuracy: 0.9982\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.0061 - accuracy: 0.9980\n",
      "10000/10000 [==============================] - 0s 47us/sample - loss: 0.0575 - accuracy: 0.9864\n",
      "0.9864\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
